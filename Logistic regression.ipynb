{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAMOYE STAGE C\n",
    "<b>LESSON 1\n",
    "    LOGISTIC REGRESSION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modified by <b><i>Miracle Osigwe</i></b></h3><br>\n",
    "Hamoye ID <b>c9013e</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>GENERAL NOTICE</h5>\n",
    "<br>\n",
    "The values on this notebook is bound to change, because random state is not set for the models<br>\n",
    "used. So don't panic if you noticed the values does not tally with what you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please read this briefly\n",
    "\n",
    "<p>I would presume you are running this on your local machine <br>\n",
    "but if peradventure, you used Google colab to open this notebook,<br>\n",
    "kindly input the url provided in cell 2 and pass it into the pandas read csv command in cell 4</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using python to download the dataset (uncomment the codes below and execute them)\n",
    "#import urllib.request\n",
    "#url = 'https://query.data.world/s/wh6j7rxy2hvrn4ml75ci62apk5hgae'\n",
    "#filename = 'dataworld.csv'\n",
    "#urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas to download the dataset (uncomment the code below and execute them)\n",
    "#df = pd.read_csv('https://query.data.world/s/wh6j7rxy2hvrn4ml75ci62apk5hgae')\n",
    "#df.to_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>country_code</th>\n",
       "      <th>record</th>\n",
       "      <th>crop_land</th>\n",
       "      <th>grazing_land</th>\n",
       "      <th>forest_land</th>\n",
       "      <th>fishing_ground</th>\n",
       "      <th>built_up_land</th>\n",
       "      <th>carbon</th>\n",
       "      <th>total</th>\n",
       "      <th>QScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>AreaPerCap</td>\n",
       "      <td>0.140292</td>\n",
       "      <td>0.199546</td>\n",
       "      <td>0.097188051</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>0.029320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.032351e-01</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>AreaTotHA</td>\n",
       "      <td>483000.000000</td>\n",
       "      <td>687000.000000</td>\n",
       "      <td>334600</td>\n",
       "      <td>127000.000000</td>\n",
       "      <td>100943.000800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.732543e+06</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>BiocapPerCap</td>\n",
       "      <td>0.159804</td>\n",
       "      <td>0.135261</td>\n",
       "      <td>0.084003213</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.262086e-01</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>BiocapTotGHA</td>\n",
       "      <td>550176.242700</td>\n",
       "      <td>465677.972200</td>\n",
       "      <td>289207.1078</td>\n",
       "      <td>47311.551720</td>\n",
       "      <td>114982.279300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467355e+06</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>EFConsPerCap</td>\n",
       "      <td>0.387510</td>\n",
       "      <td>0.189462</td>\n",
       "      <td>1.26E-06</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>1.114093</td>\n",
       "      <td>1.728629e+00</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  country  year  country_code        record      crop_land  \\\n",
       "0           0  Armenia  1992             1    AreaPerCap       0.140292   \n",
       "1           1  Armenia  1992             1     AreaTotHA  483000.000000   \n",
       "2           2  Armenia  1992             1  BiocapPerCap       0.159804   \n",
       "3           3  Armenia  1992             1  BiocapTotGHA  550176.242700   \n",
       "4           4  Armenia  1992             1  EFConsPerCap       0.387510   \n",
       "\n",
       "    grazing_land  forest_land  fishing_ground  built_up_land    carbon  \\\n",
       "0       0.199546  0.097188051        0.036888       0.029320  0.000000   \n",
       "1  687000.000000       334600   127000.000000  100943.000800  0.000000   \n",
       "2       0.135261  0.084003213        0.013742       0.033398  0.000000   \n",
       "3  465677.972200  289207.1078    47311.551720  114982.279300  0.000000   \n",
       "4       0.189462     1.26E-06        0.004165       0.033398  1.114093   \n",
       "\n",
       "          total QScore  \n",
       "0  5.032351e-01     3A  \n",
       "1  1.732543e+06     3A  \n",
       "2  4.262086e-01     3A  \n",
       "3  1.467355e+06     3A  \n",
       "4  1.728629e+00     3A  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the dataset\n",
    "df = pd.read_csv('dataset.csv', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the unnamed column (ensure you confirm it is there, else dont run this cell)\n",
    "\n",
    "df.drop('Unnamed: 0', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    51481\n",
       "2A    10576\n",
       "2B    10096\n",
       "1B       16\n",
       "1A       16\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the distribution of target variables\n",
    "df['QScore'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "The distribution of the target variables shows a huge differences between the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country               0\n",
       "year                  0\n",
       "country_code          0\n",
       "record                0\n",
       "crop_land         20472\n",
       "grazing_land      20472\n",
       "forest_land       20472\n",
       "fishing_ground    20473\n",
       "built_up_land     20473\n",
       "carbon            20473\n",
       "total                 9\n",
       "QScore                1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values in the dataset\n",
    "df.isnull().sum()\n",
    "#or\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note \n",
    "From the dataset, 20,472 rows have missing values or null values present, which would affect our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country           0\n",
       "year              0\n",
       "country_code      0\n",
       "record            0\n",
       "crop_land         0\n",
       "grazing_land      0\n",
       "forest_land       0\n",
       "fishing_ground    0\n",
       "built_up_land     0\n",
       "carbon            0\n",
       "total             0\n",
       "QScore            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for simplicity, we will drop the rows with missing values.\n",
    "df.dropna(inplace = True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "An obvious change in our target variable after removing the missing values is that there are only three classes left \n",
    "and from the distribution of the 3 classes, we can see that there is an obvious imbalance between the classes. \n",
    "There are methods that can be applied to handle this imbalance such as oversampling and undersampling.\n",
    "Oversampling involves increasing the number of instances in the class with fewer instances, in this case the class with the lower instances while undersampling involves reducing the data points in the class with more instances in this case the 3A class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the dataframe index, to correct the effect of the null values dropped from the dataset\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    51473\n",
       "2A      240\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For now, we will convert this to a binary classification problem by combining class '2A' and '1A'.\n",
    "df['QScore'] = df['QScore'].replace(['1A'], '2A')\n",
    "df.QScore.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>country_code</th>\n",
       "      <th>record</th>\n",
       "      <th>crop_land</th>\n",
       "      <th>grazing_land</th>\n",
       "      <th>forest_land</th>\n",
       "      <th>fishing_ground</th>\n",
       "      <th>built_up_land</th>\n",
       "      <th>carbon</th>\n",
       "      <th>total</th>\n",
       "      <th>QScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>AreaPerCap</td>\n",
       "      <td>2.072989e-01</td>\n",
       "      <td>8.112722e-01</td>\n",
       "      <td>0.048357265</td>\n",
       "      <td>2.258528e-02</td>\n",
       "      <td>2.998367e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.119497e+00</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>AreaTotHA</td>\n",
       "      <td>8.417600e+06</td>\n",
       "      <td>3.294260e+07</td>\n",
       "      <td>1963600</td>\n",
       "      <td>9.171000e+05</td>\n",
       "      <td>1.217520e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.545842e+07</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>BiocapPerCap</td>\n",
       "      <td>2.021916e-01</td>\n",
       "      <td>2.636077e-01</td>\n",
       "      <td>0.027166736</td>\n",
       "      <td>7.947991e-03</td>\n",
       "      <td>2.924496e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.301590e-01</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>BiocapTotGHA</td>\n",
       "      <td>8.210214e+06</td>\n",
       "      <td>1.070408e+07</td>\n",
       "      <td>1103135.245</td>\n",
       "      <td>3.227369e+05</td>\n",
       "      <td>1.187524e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.152769e+07</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>EFConsPerCap</td>\n",
       "      <td>6.280528e-01</td>\n",
       "      <td>1.810332e-01</td>\n",
       "      <td>0.162800822</td>\n",
       "      <td>1.472910e-02</td>\n",
       "      <td>2.924496e-02</td>\n",
       "      <td>1.391455e+00</td>\n",
       "      <td>2.407316e+00</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>BiocapPerCap</td>\n",
       "      <td>2.577915e+00</td>\n",
       "      <td>1.792655e+00</td>\n",
       "      <td>0.611118727</td>\n",
       "      <td>1.612259e+00</td>\n",
       "      <td>1.045676e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.698516e+00</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1984</td>\n",
       "      <td>3</td>\n",
       "      <td>AreaPerCap</td>\n",
       "      <td>2.721272e-01</td>\n",
       "      <td>2.189741e-01</td>\n",
       "      <td>0.395047858</td>\n",
       "      <td>2.203878e-01</td>\n",
       "      <td>1.624328e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.122780e+00</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>Saint Lucia</td>\n",
       "      <td>1977</td>\n",
       "      <td>189</td>\n",
       "      <td>EFProdTotGHA</td>\n",
       "      <td>3.475078e+04</td>\n",
       "      <td>1.412158e+03</td>\n",
       "      <td>3313.278393</td>\n",
       "      <td>7.791738e+03</td>\n",
       "      <td>2.939098e+02</td>\n",
       "      <td>2.490908e+04</td>\n",
       "      <td>7.247094e+04</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>1964</td>\n",
       "      <td>138</td>\n",
       "      <td>AreaTotHA</td>\n",
       "      <td>2.055000e+07</td>\n",
       "      <td>7.749900e+07</td>\n",
       "      <td>81629013.25</td>\n",
       "      <td>4.224940e+07</td>\n",
       "      <td>1.202400e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.231298e+08</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>Bolivia</td>\n",
       "      <td>1976</td>\n",
       "      <td>19</td>\n",
       "      <td>EFProdTotGHA</td>\n",
       "      <td>1.302065e+06</td>\n",
       "      <td>9.956064e+06</td>\n",
       "      <td>843714.4188</td>\n",
       "      <td>3.534220e+03</td>\n",
       "      <td>1.481085e+05</td>\n",
       "      <td>1.298080e+06</td>\n",
       "      <td>1.355157e+07</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         country  year  country_code        record     crop_land  \\\n",
       "0        Algeria  2016             4    AreaPerCap  2.072989e-01   \n",
       "1        Algeria  2016             4     AreaTotHA  8.417600e+06   \n",
       "2        Algeria  2016             4  BiocapPerCap  2.021916e-01   \n",
       "3        Algeria  2016             4  BiocapTotGHA  8.210214e+06   \n",
       "4        Algeria  2016             4  EFConsPerCap  6.280528e-01   \n",
       "..           ...   ...           ...           ...           ...   \n",
       "585    Argentina  2014             9  BiocapPerCap  2.577915e+00   \n",
       "586      Albania  1984             3    AreaPerCap  2.721272e-01   \n",
       "587  Saint Lucia  1977           189  EFProdTotGHA  3.475078e+04   \n",
       "588       Mexico  1964           138     AreaTotHA  2.055000e+07   \n",
       "589      Bolivia  1976            19  EFProdTotGHA  1.302065e+06   \n",
       "\n",
       "     grazing_land  forest_land  fishing_ground  built_up_land        carbon  \\\n",
       "0    8.112722e-01  0.048357265    2.258528e-02   2.998367e-02  0.000000e+00   \n",
       "1    3.294260e+07      1963600    9.171000e+05   1.217520e+06  0.000000e+00   \n",
       "2    2.636077e-01  0.027166736    7.947991e-03   2.924496e-02  0.000000e+00   \n",
       "3    1.070408e+07  1103135.245    3.227369e+05   1.187524e+06  0.000000e+00   \n",
       "4    1.810332e-01  0.162800822    1.472910e-02   2.924496e-02  1.391455e+00   \n",
       "..            ...          ...             ...            ...           ...   \n",
       "585  1.792655e+00  0.611118727    1.612259e+00   1.045676e-01  0.000000e+00   \n",
       "586  2.189741e-01  0.395047858    2.203878e-01   1.624328e-02  0.000000e+00   \n",
       "587  1.412158e+03  3313.278393    7.791738e+03   2.939098e+02  2.490908e+04   \n",
       "588  7.749900e+07  81629013.25    4.224940e+07   1.202400e+06  0.000000e+00   \n",
       "589  9.956064e+06  843714.4188    3.534220e+03   1.481085e+05  1.298080e+06   \n",
       "\n",
       "            total QScore  \n",
       "0    1.119497e+00     2A  \n",
       "1    4.545842e+07     2A  \n",
       "2    5.301590e-01     2A  \n",
       "3    2.152769e+07     2A  \n",
       "4    2.407316e+00     2A  \n",
       "..            ...    ...  \n",
       "585  6.698516e+00     3A  \n",
       "586  1.122780e+00     3A  \n",
       "587  7.247094e+04     3A  \n",
       "588  2.231298e+08     3A  \n",
       "589  1.355157e+07     3A  \n",
       "\n",
       "[590 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#separating the target variable and \n",
    "#undersampling the 3A class\n",
    "df_2A = df[df.QScore=='2A']\n",
    "df_3A = df[df.QScore=='3A'].sample(350)\n",
    "data_df = df_2A.append(df_3A).reset_index(drop=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    350\n",
       "2A    240\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using the scikit-learn utils function to shuffle the target variable\n",
    "import sklearn.utils\n",
    "data_df = sklearn.utils.shuffle(data_df)\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "data_df.shape\n",
    "data_df.QScore.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Note</h2>\n",
    "A reset_index call and setting the drop to True helps to reset the index and make it reflect on the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country            object\n",
       "year                int64\n",
       "country_code        int64\n",
       "record             object\n",
       "crop_land         float64\n",
       "grazing_land      float64\n",
       "forest_land        object\n",
       "fishing_ground    float64\n",
       "built_up_land     float64\n",
       "carbon            float64\n",
       "total             float64\n",
       "QScore             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the datatype of the dataset\n",
    "data_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Note</h2>\n",
    "From the above, we will be dropping country code, country and year, because they are ambiguous to what we want to predict<hr>\n",
    "And also we will encode the record feature, <br>\n",
    "Encoding is useful for columns that are needed but are not the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      5\n",
       "1      0\n",
       "2      6\n",
       "3      3\n",
       "4      0\n",
       "      ..\n",
       "585    2\n",
       "586    4\n",
       "587    6\n",
       "588    0\n",
       "589    7\n",
       "Name: record, Length: 590, dtype: int32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding the categorical feature. \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "data_df.record = encoder.fit_transform(data_df.record)\n",
    "#let's preview the encoded feature\n",
    "data_df.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing and uncorrelated columns dropping.\n",
    "data_df = data_df.drop(columns=['country_code', 'country', 'year'])\n",
    "X = data_df.drop(columns = 'QScore')\n",
    "y = data_df['QScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    231\n",
       "2A    182\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Note</h2>\n",
    "The y train is showing a value counts of 245 to 168 for 3A and 2A respectively.<br>\n",
    "which would overfit the model to make predictions towards 3A, because the model<br>\n",
    "would have more features of 3A than 2A, and consequently make it probable to focus<br> more on 3A.<br>\n",
    "To address this, we would be using imblearn to over sample our 2A class, so as to<br> create an equal feature weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing imblearn module (uncomment the pip command to install imblearn), if already installed, ignore this cell\n",
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    231\n",
       "2A    231\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There is still an imbalance in the class distribution. For this, we use SMOTE only on the training data to handle this.\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=1)\n",
    "x_train_balanced, y_balanced = smote.fit_sample(x_train, y_train)\n",
    "#value count our y_balanced to cross check the effect of smote on the dataset\n",
    "y_balanced.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using minmaxscaler to scale every other feature aside the record column\n",
    "#because the record column is not a numerical feature but a categorical one\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#scaling the x train set, without the record feature\n",
    "normalised_train_df = scaler.fit_transform(x_train_balanced.drop(columns=['record']))\n",
    "\n",
    "#passing the scaled data into a dataframe and setting the solumns without the record feature.\n",
    "#the drop statement helps to ensure that the columns were not wrongly placed.\n",
    "normalised_train_df = pd.DataFrame(normalised_train_df, columns=x_train_balanced.drop(columns=['record']).columns)\n",
    "\n",
    "#adding the record columns back\n",
    "normalised_train_df['record'] = x_train_balanced['record']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat the same for x test, \n",
    "\n",
    "#resetting the indexes for easy computatiion\n",
    "x_test = x_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "normalised_test_df = scaler.transform(x_test.drop(columns=['record']))\n",
    "normalised_test_df = pd.DataFrame(normalised_test_df, columns=x_test.drop(columns=['record']).columns)\n",
    "normalised_test_df['record'] = x_test['record']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Note</h2>\n",
    "<h4>The whole cell above were meant to prepare the data for the ML operations<br>\n",
    "which would commence from the next cell below</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(normalised_train_df, y_balanced)\n",
    "new_predictions = log_reg.predict(normalised_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l2',\n",
       " 'dual': False,\n",
       " 'tol': 0.0001,\n",
       " 'C': 1.0,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'class_weight': None,\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'verbose': 0,\n",
       " 'warm_start': False,\n",
       " 'n_jobs': None,\n",
       " 'l1_ratio': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Runnning logisticregression.__dict__\n",
    "LogisticRegression().__dict__\n",
    "#returns\n",
    "#LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "#                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "#                   multi_class='auto', n_jobs=None, penalty='l2',\n",
    "#                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "#                   warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LESSON 2</h1>\n",
    "<b>MEASURING CLASSIFICATION PERFORMANCE</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54.41176471, 50.53191489, 48.61556744, 52.92397661, 58.38095238])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross validation score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(log_reg, normalised_train_df, y_balanced, cv=5,\n",
    "                        scoring='f1_macro')\n",
    "scores*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score 54.33\n"
     ]
    }
   ],
   "source": [
    "#LeaveOneOut\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(LogisticRegression(), normalised_train_df, y_balanced, \\\n",
    "                        cv=loo, scoring='f1_macro')\n",
    "average_score = scores.mean() * 100\n",
    "print(\"Average score %.2f\" % average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual- Positive</th>\n",
       "      <th>Actual- Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted- Positive</th>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted- Negative</th>\n",
       "      <td>65</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Actual- Positive  Actual- Negative\n",
       "Predicted- Positive                31                27\n",
       "Predicted- Negative                65                54"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "cnf_mat = confusion_matrix(y_true=y_test, y_pred=new_predictions, labels=['2A', '3A'])\n",
    "\n",
    "#Preview the confusion matrix\n",
    "good_cnf = pd.DataFrame(cnf_mat, columns=['Actual- Positive', 'Actual- Negative'], \n",
    "                        index=['Predicted- Positive', 'Predicted- Negative'])\n",
    "good_cnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 is the number of True Positive (TP)\n",
      "27 is the number of False Positive (FP)\n",
      "65 is the number of False Negative (FN)\n",
      "54 is the number of True Negative (TN)\n"
     ]
    }
   ],
   "source": [
    "print(\"%.0f is the number of True Positive (TP)\" % (cnf_mat[0, 0]))\n",
    "print(\"%.0f is the number of False Positive (FP)\" % (cnf_mat[0, 1]))\n",
    "print(\"%.0f is the number of False Negative (FN)\" % (cnf_mat[1, 0]))\n",
    "print(\"%.0f is the number of True Negative (TN)\" % (cnf_mat[1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>METRICS</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.02\n"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "accuracy = accuracy_score(y_true = y_test, y_pred = new_predictions)\n",
    "print('Accuracy: {}'.format(round(accuracy*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presicion: 32.0\n"
     ]
    }
   ],
   "source": [
    "#precision\n",
    "precision = precision_score(y_true=y_test, y_pred=new_predictions, \\\n",
    "                           pos_label='2A')\n",
    "print('Presicion: {}'.format(round(precision*100), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 53.0\n"
     ]
    }
   ],
   "source": [
    "#recall\n",
    "recall = recall_score(y_true=y_test, y_pred=new_predictions, \\\n",
    "                           pos_label='2A')\n",
    "print('Recall: {}'.format(round(recall*100), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 40.0\n"
     ]
    }
   ],
   "source": [
    "#F1 scores\n",
    "f1 = f1_score(y_true=y_test, y_pred=new_predictions, pos_label='2A')\n",
    "print('F1: {}'.format(round(f1*100), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> K-FOLD </h3>\n",
    "<i>Kindly read through and let me know if you need further assistance with any section of the code</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Fold\n",
    "#calculating the classification report for kfold\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#setting the number of splits\n",
    "kf = KFold(n_splits=5)\n",
    "kf.split(normalised_train_df)\n",
    "#metric lists to hold the values calculated for every k-fold carried out\n",
    "f1_scores, accuracy_kfold, precision_kfold, recall_kfold = [], [], [], []\n",
    "kmodel = []\n",
    "\n",
    "#run for every split\n",
    "for train_index, test_index in kf.split(normalised_train_df):\n",
    "    kx_train, kx_test = normalised_train_df.iloc[train_index],\\\n",
    "                      normalised_train_df.iloc[test_index]\n",
    "    ky_train, ky_test = y_balanced[train_index],\\\n",
    "                      y_balanced[test_index]\n",
    "    model = LogisticRegression().fit(kx_train, ky_train)\n",
    "    #save result to list\n",
    "    f1_scores.append(f1_score(y_true = ky_test, y_pred = model.predict(kx_test),\n",
    "                            pos_label = '2A'))\n",
    "    accuracy_kfold.append(accuracy_score(y_true=ky_test, y_pred=model.predict(kx_test)))\n",
    "    precision_kfold.append(precision_score(y_true = ky_test, y_pred = model.predict(kx_test), \n",
    "                                           pos_label = '2A', zero_division=True))\n",
    "    recall_kfold.append(recall_score(y_true = ky_test, y_pred = model.predict(kx_test),\n",
    "                            pos_label = '2A'))\n",
    "    pred = model.predict(kx_test)\n",
    "    kmodel.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k1</th>\n",
       "      <th>k2</th>\n",
       "      <th>k3</th>\n",
       "      <th>k4</th>\n",
       "      <th>k5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.559140</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.271739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Scores</th>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 k1        k2        k3        k4        k5\n",
       "Accuracy   0.559140  0.516129  0.500000  0.500000  0.271739\n",
       "Precision  0.537037  0.510638  0.426230  0.421875  1.000000\n",
       "Recall     0.644444  0.521739  0.702703  0.750000  0.000000\n",
       "F1 Scores  0.585859  0.516129  0.530612  0.540000  0.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Passing the reports into a dataframe\n",
    "classification_scores = pd.DataFrame([accuracy_kfold, precision_kfold, recall_kfold, f1_scores], index=['Accuracy', \n",
    "                                                                                                          'Precision',\n",
    "                                                                                                          'Recall',\n",
    "                                                                                                         'F1 Scores'],\n",
    "                                    columns=['k1', 'k2', 'k3', 'k4', 'k5'])\n",
    "classification_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Note</h2>\n",
    "<h3>CLASSIFICATION REPORT</h3>\n",
    "Classification report is useful for models that uses classifiers for it's algorithms.<br>\n",
    "<p></p>\n",
    "Try comparing the metrics given by the classification report and the ones given by each<br>\n",
    "metrics, you will notice a difference in value.<br>\n",
    "Should incase you have an explanation to this, kindly use the comment section to help clarify this<br>\n",
    "Also all the metrics used from this point downward are in decimal places and not percentage, for easy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          2A       0.79      0.63      0.70        67\n",
      "          3A       0.36      0.56      0.44        25\n",
      "\n",
      "    accuracy                           0.61        92\n",
      "   macro avg       0.58      0.59      0.57        92\n",
      "weighted avg       0.67      0.61      0.63        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "if len(ky_test) != len(kmodel[0]):\n",
    "    kmodels = kmodel[0][:-1]\n",
    "    print(classification_report(ky_test, kmodels))\n",
    "else:\n",
    "    print(classification_report(ky_test, kmodel[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          2A       0.72      0.49      0.58        67\n",
      "          3A       0.26      0.48      0.34        25\n",
      "\n",
      "    accuracy                           0.49        92\n",
      "   macro avg       0.49      0.49      0.46        92\n",
      "weighted avg       0.59      0.49      0.52        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "if len(ky_test) != len(kmodel[1]):\n",
    "    kmodels = kmodel[1][1:]\n",
    "    print(classification_report(ky_test, kmodels))\n",
    "else:\n",
    "    print(classification_report(ky_test, kmodel[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can experiment for other splits by using the slice (2, 3, 4) inside the previous classification reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stratified K-FOLD</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StratifiedKFold\n",
    "#Calculating the classification reports skfold \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "f1_scores_skfold, accuracy_skfold, precision_skfold, recall_skfold, cnf_mtxS = [], [], [], [], []\n",
    "skmodel=[]\n",
    "\n",
    "#run for every split\n",
    "for train_index, test_index in skf.split(normalised_train_df, y_balanced):\n",
    "    skx_train, skx_test = np.array(normalised_train_df)[train_index],\\\n",
    "                      np.array(normalised_train_df)[test_index]\n",
    "    sky_train, sky_test = y_balanced[train_index],\\\n",
    "                      y_balanced[test_index]\n",
    "    model = LogisticRegression().fit(skx_train, sky_train)\n",
    "    #save result to list\n",
    "    f1_scores_skfold.append(f1_score(y_true = sky_test, y_pred = model.predict(skx_test),\n",
    "                            pos_label = '2A'))\n",
    "    accuracy_skfold.append(accuracy_score(y_true=sky_test, y_pred=model.predict(skx_test)))\n",
    "    precision_skfold.append(precision_score(y_true = sky_test, y_pred = model.predict(skx_test), \n",
    "                                            pos_label = '2A', zero_division=True))\n",
    "    recall_skfold.append(recall_score(y_true = sky_test, y_pred = model.predict(skx_test),\n",
    "                            pos_label = '2A'))\n",
    "    skmodel.append(model.predict(skx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sk1</th>\n",
       "      <th>sk2</th>\n",
       "      <th>sk3</th>\n",
       "      <th>sk4</th>\n",
       "      <th>sk5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.641304</td>\n",
       "      <td>0.423913</td>\n",
       "      <td>0.510870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.508772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.630435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Scores</th>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.563107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sk1       sk2       sk3       sk4       sk5\n",
       "Accuracy   0.537634  0.591398  0.641304  0.423913  0.510870\n",
       "Precision  0.533333  0.576923  0.638298  0.410256  0.508772\n",
       "Recall     0.680851  0.652174  0.652174  0.347826  0.630435\n",
       "F1 Scores  0.598131  0.612245  0.645161  0.376471  0.563107"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Passing the values into a dataframe\n",
    "classification_scores2 = pd.DataFrame([accuracy_skfold, precision_skfold, recall_skfold, f1_scores_skfold], index=['Accuracy', \n",
    "                                                                                                          'Precision',\n",
    "                                                                                                          'Recall',\n",
    "                                                                                                         'F1 Scores'],\n",
    "                                    columns=['sk1', 'sk2', 'sk3', 'sk4', 'sk5'])\n",
    "classification_scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          2A       0.47      0.61      0.53        46\n",
      "          3A       0.44      0.30      0.36        46\n",
      "\n",
      "    accuracy                           0.46        92\n",
      "   macro avg       0.45      0.46      0.44        92\n",
      "weighted avg       0.45      0.46      0.44        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "if len(sky_test) != len(skmodel[0]):\n",
    "    skmodels = skmodel[0][:-1]\n",
    "    print(classification_report(sky_test, skmodels))\n",
    "else:\n",
    "    print(classification_report(sky_test, skmodel[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          2A       0.46      0.52      0.49        46\n",
      "          3A       0.45      0.39      0.42        46\n",
      "\n",
      "    accuracy                           0.46        92\n",
      "   macro avg       0.46      0.46      0.45        92\n",
      "weighted avg       0.46      0.46      0.45        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "if len(sky_test) != len(skmodel[1]):\n",
    "    skmodels = skmodel[1][1:]\n",
    "    print(classification_report(sky_test, skmodels))\n",
    "else:\n",
    "    print(classification_report(sky_test, skmodel[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can experiment for other splits by using the slice (2, 3, 4) inside the previous classification reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>DECISION TREE CLASSIFIER</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree-Based Methods and The Support Vector Machine\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "dec_tree.fit(normalised_train_df, y_balanced)\n",
    "dec_pred = dec_tree.predict(normalised_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.599\n",
      "Precision score for label 2A 0.393\n",
      "Recall score for label 2A 0.414\n",
      "F1 score 0.403\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score {}\".format(round(accuracy_score(y_test, dec_pred), 3)))\n",
    "print(\"Precision score for label 2A %.3f\" % (precision_score(y_test, dec_pred, pos_label='2A')))\n",
    "print(\"Recall score for label 2A {}\".format(round(recall_score(y_test, dec_pred, pos_label='2A'), 3)))\n",
    "print(\"F1 score %.3f\" % (f1_score(y_test, dec_pred, pos_label='2A')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          2A       0.39      0.41      0.40        58\n",
      "          3A       0.71      0.69      0.70       119\n",
      "\n",
      "    accuracy                           0.60       177\n",
      "   macro avg       0.55      0.55      0.55       177\n",
      "weighted avg       0.60      0.60      0.60       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, dec_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>WORKING WITH XGBOOST AND LIGHTGBM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing two xgboost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>XGBRFCLASSIFIER</h2>\n",
    "XGBoost is normally used to train gradient-boosted decision trees<br> \n",
    "and other gradient boosted models. Random forests use the same model<br> \n",
    "representation and inference, as gradient-boosted decision trees,<br> \n",
    "but a different training algorithm. One can use XGBoost to train a<br> \n",
    "standalone random forest or use random forest as a base model for<br> \n",
    "gradient boosting. Here we focus on training standalone random forest.\n",
    "\n",
    "<h2>XGBCLASSIFIER</h2>\n",
    "XGBoost is a decision-tree-based ensemble Machine Learning algorithm<br> \n",
    "that uses a gradient boosting framework. In prediction problems involving<br> \n",
    "unstructured data (images, text, etc.) artificial neural networks tend to<br> \n",
    "outperform all other algorithms or frameworks. However, when it comes to<br> \n",
    "small-to-medium structured/tabular data, decision tree based algorithms are<br> \n",
    "considered best-in-class right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost with random forest\n",
    "from xgboost import XGBRFClassifier\n",
    "extreme = XGBRFClassifier()\n",
    "extreme.fit(normalised_train_df, y_balanced)\n",
    "extreme_pred = extreme.predict(normalised_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          2A       0.50      0.66      0.57        58\n",
      "          3A       0.80      0.68      0.74       119\n",
      "\n",
      "    accuracy                           0.67       177\n",
      "   macro avg       0.65      0.67      0.65       177\n",
      "weighted avg       0.70      0.67      0.68       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test, extreme_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost using gradient boosting\n",
    "from xgboost import XGBClassifier\n",
    "extreme1 = XGBClassifier()\n",
    "extreme1.fit(normalised_train_df, y_balanced)\n",
    "extreme1_pred = extreme1.predict(normalised_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          2A       0.51      0.62      0.56        58\n",
      "          3A       0.79      0.71      0.75       119\n",
      "\n",
      "    accuracy                           0.68       177\n",
      "   macro avg       0.65      0.67      0.66       177\n",
      "weighted avg       0.70      0.68      0.69       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test, extreme1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "light = LGBMClassifier()\n",
    "light.fit(normalised_train_df, y_balanced)\n",
    "light_pred = light.predict(normalised_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          2A       0.49      0.59      0.53        58\n",
      "          3A       0.78      0.70      0.73       119\n",
      "\n",
      "    accuracy                           0.66       177\n",
      "   macro avg       0.63      0.64      0.63       177\n",
      "weighted avg       0.68      0.66      0.67       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test, light_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>EXTRA TREE CLASSIFIER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra tree classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "tree = ExtraTreesClassifier()\n",
    "tree.fit(normalised_train_df, y_balanced)\n",
    "tree_pred = tree.predict(normalised_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tree tuning using hyper parameters setting</h3>\n",
    "\n",
    "Now to improve the Extra Trees Classifier, you will use the following parameters<br> \n",
    "<ul>\n",
    "    <ol>Number of estimators</ol> \n",
    "    <ol>Minimum number of samples,</ol> \n",
    "    <ol>Minimum number of samples for leaf node and </ol>\n",
    "    <ol>The number of features to consider when looking for the best split</ol>\n",
    "</ul>    \n",
    "For the hyperparameter grid needed to run a Randomized Cross Validation Search (RandomizedSearchCV). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 50, 100, 250, 400]\n",
    "min_samples_split = [2, 3, 5, 7, 9]\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "max_features = ['auto', 'sqrt', 'log2', None] \n",
    "hyperparameter_grid = {'n_estimators': n_estimators,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'max_features': max_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "clf = RandomizedSearchCV(tree, hyperparameter_grid, random_state=0)\n",
    "search = clf.fit(normalised_train_df, y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([400, 2, 1, 'log2'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for the best parameter for the model\n",
    "search.best_params_.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experimenting with this parameter to test the model's performance\n",
    "tree_param = ExtraTreesClassifier(n_estimators=400, min_samples_split=2, \n",
    "                                 min_samples_leaf=1, max_features='log2')\n",
    "tree_param.fit(normalised_train_df, y_balanced)\n",
    "tree_param_pred = tree_param.predict(normalised_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          2A       0.29      0.28      0.28        58\n",
      "          3A       0.65      0.66      0.66       119\n",
      "\n",
      "    accuracy                           0.54       177\n",
      "   macro avg       0.47      0.47      0.47       177\n",
      "weighted avg       0.53      0.54      0.53       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report for this hyperparameter tuning\n",
    "print(classification_report(y_test, tree_param_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classification Report clarification</h2>\n",
    "\n",
    "Please use the comment section to help clarify these reports and their significances.<br>\n",
    "Thanks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NOTE</h1>\n",
    "<h3>IF THIS NOTEBOOK WAS HELPFUL, KINDLY VOTE FOR ME USING THESE DETAILS</h3>\n",
    "<h2> Name: OSIGWE MIRACLE</h2>\n",
    "<h3>ID: c9013e</h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
